{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yG_n40gFzf9s",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 18:22:56.644753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-28 18:22:57.002802: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:22:57.002851: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-12-28 18:22:58.166595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:22:58.166761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:22:58.166774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOURCE TEXT LOADING\n",
    "- loading .txt\n",
    "- printing the beginning, number of characters, number of unique characters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aavnuByVymwK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE  FELLOWSHIP \n",
      "OF  THE  RING \n",
      "\n",
      "\n",
      "BEING  THE  FIRST  PART \n",
      "OF \n",
      "\n",
      "The  Lord  of  the  Rings \n",
      "\n",
      "\n",
      "BOOK  ONE \n",
      "\n",
      "\n",
      "Chapter  I \n",
      "\n",
      "\n",
      "A  LONG-EXPECTED  PARTY \n",
      "\n",
      "\n",
      "When  Mr.  Bilbo  Baggins  of  Bag  End  announced  that  he \n",
      "would  shortly  be  celebrating  his  eleventy-first  birthday  with \n",
      "a  party  of  special\n"
     ]
    }
   ],
   "source": [
    "with open(\"ring.txt\", \"r\") as f:\n",
    "    source = f.read()\n",
    "print(source[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Duhg9NrUymwO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145534 characters\n"
     ]
    }
   ],
   "source": [
    "print ('{} characters'.format(len(source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IlCgQBRVymwR",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(source))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IalZLbvOzf-F",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 33 30 ...  1  0  0]\n"
     ]
    }
   ],
   "source": [
    "# unique characters to indices mapping\n",
    "char2index = {u:i for i, u in enumerate(vocab)}\n",
    "index2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2index[c] for c in source])\n",
    "\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l1VKcQHcymwb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'THE  FELLOWSH' -- characters mapped to int -- > [45 33 30  1  1 31 30 37 37 40 48 44 33]\n"
     ]
    }
   ],
   "source": [
    "print ('{} -- characters mapped to int -- > {}'.format(repr(source[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET PREPROCESSING\n",
    "- character to index mapping and converting text to numerical representation\n",
    "- defining max input sequence length\n",
    "- creating TensorFlow Dataset from the source\n",
    "- creating batches from dataset\n",
    "- input-target split function (:-1 is input, and last element is expected value)\n",
    "- applying input-target split function to each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0UHJDA39zf-O",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "H\n",
      "E\n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 18:23:01.549537: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549640: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549694: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549787: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549829: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-12-28 18:23:01.549926: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-12-28 18:23:01.552384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(source)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(index2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l4hkDU3i7ozi",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'THE  FELLOWSHIP \\nOF  THE  RING \\n\\n\\nBEING  THE  FIRST  PART \\nOF \\n\\nThe  Lord  of  the  Rings \\n\\n\\nBOOK  ON'\n",
      "'E \\n\\n\\nChapter  I \\n\\n\\nA  LONG-EXPECTED  PARTY \\n\\n\\nWhen  Mr.  Bilbo  Baggins  of  Bag  End  announced  tha'\n",
      "'t  he \\nwould  shortly  be  celebrating  his  eleventy-first  birthday  with \\na  party  of  special  m'\n",
      "'agnificence,  there  was  much  talk  and \\nexcitement  in  Hobbiton. \\n\\nBilbo  was  very  rich  and  v'\n",
      "'ery  peculiar,  and  had  been  the \\nwonder  of  the  Shire  for  sixty  years,  ever  since  his  re'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(index2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9NGu-FkO_kYU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS\n",
    "- training parameters & data shuffling\n",
    "- model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "p2pGotuNzf-S",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(2048, 100), dtype=tf.int64, name=None), TensorSpec(shape=(2048, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zHT8cLh7EAsg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 300 #256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units1 = 256\n",
    "rnn_units2 = 256\n",
    "rnn_units=[rnn_units1, rnn_units2]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING MODEL\n",
    "- GRU, LSTM, RNN, with Conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MtCrdfzEI2N0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ORIGINAL GRU\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units1,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.GRU(rnn_units2,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.LSTM(rnn_units1,\n",
    "#                              return_sequences=True,\n",
    "#                              stateful=True,\n",
    "#                              recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.LSTM(rnn_units2,\n",
    "#                              return_sequences=True,\n",
    "#                              stateful=True,\n",
    "#                              recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.SimpleRNN(rnn_units1,\n",
    "#                                   return_sequences=True,\n",
    "#                                   stateful=True,\n",
    "#                                   recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.SimpleRNN(rnn_units2,\n",
    "#                                   return_sequences=True,\n",
    "#                                   stateful=True,\n",
    "#                                   recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dodanie Conv1\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.Conv1D(filters=128, \n",
    "#                                kernel_size=5, \n",
    "#                                activation='relu', \n",
    "#                                padding='same'),\n",
    "#         tf.keras.layers.GRU(rnn_units1,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.GRU(rnn_units2,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wwsrpOik5zhv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vPGmAAXmVLGC",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (2048, None, 300)         26400     \n",
      "                                                                 \n",
      " gru (GRU)                   (2048, None, 256)         428544    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (2048, None, 256)         394752    \n",
      "                                                                 \n",
      " dense (Dense)               (2048, None, 88)          22616     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 872,312\n",
      "Trainable params: 872,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING\n",
    "- optimizer & loss function declaration\n",
    "- compilation\n",
    "- epochs number\n",
    "- launching the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4HrXTACTdzY-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DDl1_Een6rL0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "W6fWTriUZP-n",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints_GRU'        # TODO: remember to choose proper directory\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7yGBE2zxMMHs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UK-hmKjYVoll",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 27s 5s/step - loss: 4.2794 - accuracy: 0.2457\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 26s 5s/step - loss: 3.4697 - accuracy: 0.3008\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 26s 5s/step - loss: 3.1727 - accuracy: 0.3009\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 24s 5s/step - loss: 2.9621 - accuracy: 0.3009\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 28s 6s/step - loss: 2.9341 - accuracy: 0.3008\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "- bulding the model and applying saved weights\n",
    "- text generation function\n",
    "- generate text based on the provided beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zk2WJ2-XjkGz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "latest_check= tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LycQ-ot_jjyu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(latest_check)\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "71xa6jnYVrAN",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 300)            26400     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (1, None, 256)            428544    \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (1, None, 256)            394752    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 88)             22616     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 872,312\n",
      "Trainable params: 872,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WvuwZBX5Ogfd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "  num_generate = 1000\n",
    "  input_eval = [char2index[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  text_generated = []\n",
    "  scaling = 0.5\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      predictions = predictions / scaling\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(index2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry killed Ron  e  e  s e   a       e  eii                           a h    e      ra    a h    e              e se t ea r       i  n   aa   e n              d       i   d   l       e ie       s  r         r             h  \n",
      "e   t    am          ld  a      l   s n  gde    t n ae        w   ad n e i s  ss         s  d l       lee i   a  s  stt         a  oi   h s e     e srgt  ae  o  a    e      d  ,edr     n o  inm   enia ai  a r s t  dh                              an    d   u a    i  hi    e      wo      r              ss   o t   a a u selelndas \n",
      " rn    tr     a             n e e       pt  \n",
      "  n     r l        o a  etdh s      le   on     il     e s ru e e     l n   l  as na     ll y    he ee          a   es  i   ta  h             a  d      n  m    le\n",
      "e   i d   d    r d          n a  rn   e  u     s n   e       i  o  ra tn     h d t  a  h        r    e o         n       a   e amor es e i      ln  a \n",
      " o  ms h   e    n   t i e      a          w      \n",
      "e      a  s  ed         l   \n",
      "e\n",
      "     n s  a u    e d \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Harry killed Ron \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchaster Uniter just lost the war on Anfield fighting m  ad  ertw       s      ud e  s  h  in  r        t      e  a    hag    e      e or    r    et    a i e        l  e                  e er     l y     t  t       i  r  o          att       ewas      n     e      n  h w a o       n  t    ,h      m    ae            nsa      . d     d     y  e      h  r n  e     a e  yild   s  ie   d   edd neg   l rae a  te   a i     se a       h       ede a l     eh  a  anr  d eh   e    ee   nlea d   o      i         e   n dt   s      at  e  emi   e  s          tfe      r  d sh       so    n   ae e            dett            e s  es   \n",
      "  r    a   o e   i   i           e  n n d sdt  oe   i         o      f        i e      s     ie i m         iops  e  t      s    \n",
      "  \n",
      " e      wt   u  e          a\n",
      "  l os    d an     a    r   r d e  a n r  e   si     es rl uee de      n   s     i e id e     e      it m   el         n  re          n m   n   se   a s   nn  r  ye  n e  c  y         e    t i.  y.    w  e  i t  hoe   s r nwl      n   eeo d  e e     sh  t  nn  e  \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Manchaster Uniter just lost the war on Anfield fighting\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cross_entropy_loss(model, test_sequences):\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for sequence in test_sequences:\n",
    "        input_seq = sequence[:-1]\n",
    "        target_seq = sequence[1:]\n",
    "        \n",
    "        input_seq = tf.expand_dims(input_seq, 0)\n",
    "        \n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(target_seq, predictions[0], from_logits=True)\n",
    "        \n",
    "        total_loss += tf.reduce_sum(loss).numpy()\n",
    "        total_steps += len(target_seq)\n",
    "    \n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutaj można załadować inną część tolkiena albo hobbita\n",
    "first_10_sequences = list(sequences.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.331626892089844"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss = calculate_cross_entropy_loss(model, first_10_sequences)\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.892457089374666"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = np.exp(avg_loss)\n",
    "perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
