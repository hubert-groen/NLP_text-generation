{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "yG_n40gFzf9s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOURCE TEXT LOADING\n",
    "- loading .txt\n",
    "- printing the beginning, number of characters, number of unique characters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "aavnuByVymwK",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "\n",
      "\n",
      "CHAPTER ONE\n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last\n",
      "people you'd expect to be involved in anything strange or mysterious,\n",
      "because they ju\n"
     ]
    }
   ],
   "source": [
    "source = open('stone.txt').read()\n",
    "print(source[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "Duhg9NrUymwO",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439742 characters\n"
     ]
    }
   ],
   "source": [
    "print ('{} characters'.format(len(source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "IlCgQBRVymwR",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(source))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "IalZLbvOzf-F",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 52 69 ... 38 28  1]\n"
     ]
    }
   ],
   "source": [
    "# unique characters to indices mapping\n",
    "char2index = {u:i for i, u in enumerate(vocab)}\n",
    "index2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2index[c] for c in source])\n",
    "\n",
    "print(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "l1VKcQHcymwb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Harry Potter ' -- characters mapped to int -- > [32 52 69 69 76  2 40 66 71 71 56 69  2]\n"
     ]
    }
   ],
   "source": [
    "print ('{} -- characters mapped to int -- > {}'.format(repr(source[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET PREPROCESSING\n",
    "- character to index mapping and converting text to numerical representation\n",
    "- defining max input sequence length\n",
    "- creating TensorFlow Dataset from the source\n",
    "- creating batches from dataset\n",
    "- input-target split function (:-1 is input, and last element is expected value)\n",
    "- applying input-target split function to each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "0UHJDA39zf-O",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "a\n",
      "r\n",
      "r\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(source)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(index2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "id": "l4hkDU3i7ozi",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\\n\\nMr. and Mrs. Dursley, of numb\"\n",
      "'er four, Privet Drive, were proud to say\\nthat they were perfectly normal, thank you very much. They w'\n",
      "\"ere the last\\npeople you'd expect to be involved in anything strange or mysterious,\\nbecause they just \"\n",
      "\"didn't hold with such nonsense.\\n\\nMr. Dursley was the director of a firm called Grunnings, which made\\n\"\n",
      "'drills. He was a big, beefy man with hardly any neck, although he did\\nhave a very large mustache. Mrs'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(index2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "9NGu-FkO_kYU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS\n",
    "- training parameters & data shuffling\n",
    "- model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "p2pGotuNzf-S",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "zHT8cLh7EAsg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 300 #256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units1 = 1024\n",
    "rnn_units2 = 1024\n",
    "rnn_units=[rnn_units1, rnn_units2]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING MODEL\n",
    "- GRU, LSTM, RNN, with Conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "MtCrdfzEI2N0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ORIGINAL GRU\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units1,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.GRU(rnn_units2,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.LSTM(rnn_units1,\n",
    "#                              return_sequences=True,\n",
    "#                              stateful=True,\n",
    "#                              recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.LSTM(rnn_units2,\n",
    "#                              return_sequences=True,\n",
    "#                              stateful=True,\n",
    "#                              recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.SimpleRNN(rnn_units1,\n",
    "#                                   return_sequences=True,\n",
    "#                                   stateful=True,\n",
    "#                                   recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.SimpleRNN(rnn_units2,\n",
    "#                                   return_sequences=True,\n",
    "#                                   stateful=True,\n",
    "#                                   recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dodanie Conv1\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.Conv1D(filters=128, \n",
    "#                                kernel_size=5, \n",
    "#                                activation='relu', \n",
    "#                                padding='same'),\n",
    "#         tf.keras.layers.GRU(rnn_units1,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.GRU(rnn_units2,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "wwsrpOik5zhv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "vPGmAAXmVLGC",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (64, None, 300)           23700     \n",
      "                                                                 \n",
      " gru_20 (GRU)                (64, None, 1024)          4073472   \n",
      "                                                                 \n",
      " gru_21 (GRU)                (64, None, 1024)          6297600   \n",
      "                                                                 \n",
      " dense_20 (Dense)            (64, None, 79)            80975     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10475747 (39.96 MB)\n",
      "Trainable params: 10475747 (39.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING\n",
    "- optimizer & loss function declaration\n",
    "- compilation\n",
    "- epochs number\n",
    "- launching the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "4HrXTACTdzY-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "DDl1_Een6rL0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "W6fWTriUZP-n",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints_GRU'        # TODO: remember to choose proper directory\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "7yGBE2zxMMHs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "UK-hmKjYVoll",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 186s 3s/step - loss: 3.1622 - accuracy: 0.1967\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "- bulding the model and applying saved weights\n",
    "- text generation function\n",
    "- generate text based on the provided beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "id": "zk2WJ2-XjkGz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "latest_check= tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "id": "LycQ-ot_jjyu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(latest_check)\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "71xa6jnYVrAN",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    (1, None, 300)            23700     \n",
      "                                                                 \n",
      " gru_22 (GRU)                (1, None, 1024)           4073472   \n",
      "                                                                 \n",
      " gru_23 (GRU)                (1, None, 1024)           6297600   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (1, None, 79)             80975     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10475747 (39.96 MB)\n",
      "Trainable params: 10475747 (39.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "WvuwZBX5Ogfd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "  num_generate = 1000\n",
    "  input_eval = [char2index[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  text_generated = []\n",
    "  scaling = 0.5\n",
    "\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      predictions = predictions / scaling\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      text_generated.append(index2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry killed Ron yhe bat he thh an ose sole sarere the wat ot he ole pame and the fas tone an tol the th we tot he to ter yoroy sarey om bor the silad woron werind tor oring an to the sare soros the woud gepad there sas toud the tod the thin thor bore ere foin wemot he wose wat an oros an an he lad. on ant the sos the finle wher the ang onon wead anle sot oud soo fed an the lroind hinl herg and tint an the tor\n",
      "ber momet an the eod an yote yin lad an the tounr and lo wat tiuns whal the be tal oud oo fan the the the tol onus thar he thore sing tor the iny ang thele was eard alind the tacbe ad car od the fort ant for sor the the the the sas boring the the sot ang eres thed war the the pe the ferere forer the sos she the douns fead the tom sat arkeinnd sole soun les an meree sate ne won cerelr ant and anl the un ther he sot oul thete there yed sas aly ind sarg an were tot ouny saok ke mals sare wapre te ton lek aned she an he th the fint he sorethe fare the the ther anr and the sared tot care orere thit th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Harry killed Ron \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni Cross-Entropy Loss na zbiorze testowym: 5.3557\n"
     ]
    }
   ],
   "source": [
    "def calculate_cross_entropy_loss(model, test_sequences):\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for sequence in test_sequences:\n",
    "        input_seq = sequence[:-1]\n",
    "        target_seq = sequence[1:]\n",
    "        \n",
    "        input_seq = tf.expand_dims(input_seq, 0)\n",
    "        \n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(target_seq, predictions[0], from_logits=True)\n",
    "        \n",
    "        total_loss += tf.reduce_sum(loss).numpy()\n",
    "        total_steps += len(target_seq)\n",
    "    \n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutaj można załadować inną część tolkiena albo hobbita\n",
    "first_10_sequences = list(sequences.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8558943939208983"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss = calculate_cross_entropy_loss(model, first_10_sequences)\n",
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.38998375000394"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = np.exp(avg_loss)\n",
    "perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
